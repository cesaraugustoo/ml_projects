{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from typing import List, Dict, Any\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Import custom modules\n",
    "# from graph_gen import KnowledgeGraphBuilder, GraphConfig\n",
    "# from llm_tools import OpenAIClient, OpenAIConfig, create_generate_fn\n",
    "import sys\n",
    "sys.path.append('')\n",
    "# from graph_reasoning import *\n",
    "from graph_reasoning.graph_gen import KnowledgeGraphBuilder, GraphConfig\n",
    "from graph_reasoning.llm_tools import GeminiClient, GeminiConfig, create_generate_fn\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set up output directory\n",
    "OUTPUT_DIR = Path(\"\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ArxivConfig:\n",
    "    \"\"\"Configuration for ArXiv data processing.\"\"\"\n",
    "    input_file: Path = Path(\"data/arxiv-metadata-oai-snapshot.json\")\n",
    "    filtered_file: Path = Path(\"data/filtered_arxiv.json\")\n",
    "    categories: List[str] = field(default_factory=lambda: [\"physics.optics\"])\n",
    "    max_papers: int = 100  # Limit for testing\n",
    "\n",
    "# LLM configuration\n",
    "api_key = getpass(\"Please enter your API key: \")\n",
    "if not api_key.strip():\n",
    "    raise ValueError(\"API key cannot be empty.\")\n",
    "\n",
    "gemini_config = GeminiConfig(\n",
    "    api_key=api_key,\n",
    "    max_tokens=8192,\n",
    "    temperature=0,\n",
    "    model_name = \"gemini-2.0-flash-exp\",\n",
    "    top_k  = 40,\n",
    "    top_p = 0.95,\n",
    ")\n",
    "\n",
    "# Graph generation configuration\n",
    "graph_config = GraphConfig(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    system_prompt=\"\"\"\n",
    "    Analyze the scientific abstract and extract key concepts and their relationships.\n",
    "    Return a JSON object containing edges between concepts. Each edge should have:\n",
    "    - source: The source concept/entity\n",
    "    - target: The target concept/entity\n",
    "    - attributes: Including relationship type and confidence score\n",
    "    Focus on:\n",
    "    - Technical terms and concepts\n",
    "    - Experimental methods and results\n",
    "    - Theoretical frameworks\n",
    "    - Cause-effect relationships\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_arxiv(config: ArxivConfig) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load and filter ArXiv papers based on categories.\n",
    "    \"\"\"\n",
    "    filtered_papers = []\n",
    "    \n",
    "    logger.info(f\"Loading papers from {config.input_file}\")\n",
    "    with open(config.input_file, 'r') as f:\n",
    "        for i, line in enumerate(tqdm(f)):\n",
    "            if i >= config.max_papers:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                paper = json.loads(line)\n",
    "                if any(cat in paper.get('categories', '') \n",
    "                      for cat in config.categories):\n",
    "                    filtered_papers.append({\n",
    "                        'id': paper.get('id'),\n",
    "                        'title': paper.get('title'),\n",
    "                        'abstract': paper.get('abstract'),\n",
    "                        'categories': paper.get('categories')\n",
    "                    })\n",
    "            except json.JSONDecodeError:\n",
    "                logger.warning(f\"Could not parse line {i}\")\n",
    "                continue\n",
    "    \n",
    "    logger.info(f\"Found {len(filtered_papers)} papers in specified categories\")\n",
    "    return filtered_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_papers(papers: List[Dict[str, Any]], output_dir: Path):\n",
    "    \"\"\"\n",
    "    Process papers and generate knowledge graph.\n",
    "    \"\"\"\n",
    "    # Initialize AI client and create generate_fn\n",
    "    ai_client = GeminiClient(gemini_config)\n",
    "    generate_fn = create_generate_fn(ai_client)\n",
    "    \n",
    "    # Initialize graph builder\n",
    "    graph_builder = KnowledgeGraphBuilder(\n",
    "        config=graph_config,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    # Process each paper\n",
    "    for paper in tqdm(papers, desc=\"Processing papers\"):\n",
    "        try:\n",
    "            # Combine title and abstract for context\n",
    "            text = f\"Title: {paper['title']}\\n\\nAbstract: {paper['abstract']}\"\n",
    "            \n",
    "            # Generate graph for this paper\n",
    "            graph, embeddings = graph_builder.build_graph_from_text(\n",
    "                text=text,\n",
    "                generate_fn=generate_fn,\n",
    "                graph_root=f\"paper_{paper['id']}\"\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Successfully processed paper {paper['id']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing paper {paper['id']}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize config\n",
    "    config = ArxivConfig(    \n",
    "        input_file=\"\",\n",
    "        filtered_file = \"\",\n",
    "        categories = \"\",\n",
    "        max_papers = 3\n",
    "    )\n",
    "    \n",
    "    # Load and filter papers\n",
    "    papers = load_and_filter_arxiv(config)\n",
    "    \n",
    "    # Process papers and generate knowledge graphs\n",
    "    process_papers(papers, OUTPUT_DIR)\n",
    "    \n",
    "    logger.info(\"Processing complete. Check the output directory for results.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
